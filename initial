---
title: "Assignment 3"
author: "Elisabeth Fidrmuc"
date: "2023-05-12"
output: pdf_document
---

```{r setup,results='hide',warning=FALSE,message=FALSE}
library(rstanarm)
library(stargazer)
library(bayesplot)
```

# Exercise 1

```{r}
rugged <- read.csv("C:/Users/elisa/Dropbox/Advanced Macroeconometrics/Homework 3/rugged_data.csv")
head(rugged)[,c("rgdppc_2000","rugged","dist_coast","cont_africa","pop_1400")]
```

(@) Reproduce the main result, given in Table 1, Column 5 (they use HC1 standard errors), of Nunn and
Puga (2012) by fitting a model with the following (interacted) variables:

$$\log rgdppc_{2000} \approx (rugged+dist-coast) \times africa$$


```{r,results='hide'}
clm1 <- lm(log(rgdppc_2000)~(rugged+dist_coast)*cont_africa,data=rugged)
blm1 <- stan_glm(log(rgdppc_2000)~(rugged+dist_coast)*cont_africa,data=rugged, seed=111)
table1 <- round(cbind(coef(clm1),coef(blm1)),3)
colnames(table1) <- c("Classical Linear Regression","Bayesian Regression")
```


```{r,results='asis',message=FALSE}
stargazer(table1,out="Table 1.html")
```

We get very similar results to the original table both for the classical linear model as well as for the Bayesian estimation.

(@) Plot posterior samples of the ruggedness effect in Africa against the effect in the rest of the world in a scatterplot. What can you say about the effect?

```{r}
posterior_sample <- as.data.frame(blm1)
plot(posterior_sample$rugged,posterior_sample$`rugged:cont_africa`,
     xlab = "Ruggedness Effect in Rest of the World",
     ylab = "Ruggedness Effect in Africa",
     main = "Scatterplot of Ruggedness Effects",
     pch = 16)
```

The ruggedness effect in Africa and that in the rest of the world seem to be negatively correlated. A larger ruggedness effect in the rest of the world goes in hand with a smaller one in Africa.

(@) Estimate three additional models â€” one without the distance to coast, one that uses population in 1400 (use log 1+pop) instead, and one with both controls.


```{r,results='hide'}
blm2 <- stan_glm(log(rgdppc_2000)~rugged*cont_africa,data=rugged, seed=111)
blm3 <- stan_glm(log(rgdppc_2000)~(rugged+log(1+pop_1400))*cont_africa,data=rugged, seed=111)
blm4 <- stan_glm(log(rgdppc_2000)~(rugged+dist_coast+log(1+pop_1400))*cont_africa,data=rugged, seed=111)

labels <- row.names(cbind(coef(blm4)))
Model1 <- c(coef(blm1)[1:3],NA,coef(blm1)[4:6],NA)
Model2 <- c(coef(blm2)[1:2],NA,NA,coef(blm2)[3:4],NA,NA)
Model3 <- c(coef(blm3)[1:2],NA,coef(blm3)[3:5],NA,coef(blm3)[6])
table2 <- round(cbind(Model1,Model2,Model3,Model4=coef(blm4)),3)
rownames(table2) <- labels
```

```{r,results='asis',message=FALSE}
stargazer(table2,out="Table 2.html")
```


(@)  Discuss (conceptually different) approaches to selecting one of these models for inference. Hint: Consider the difference between causal inference and other inference tasks.

There are several ways to select the "best" models, such as

- **Bayesian model comparison:** Use the Widely Applicable Information Criterion (WAIC) or the Leave-One-Out Cross-Validation (LOO) to compare the models' predictive performance. Lower values of WAIC or LOO indicate better model fit. These methods account for both model complexity and fit to the data.
- **Model selection criteria:** Use other model selection criteria like the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). Lower AIC or BIC values indicate better model fit, but these criteria do not account for model complexity as well as WAIC or LOO.
_ **Visual inspection of results:** Examine the posterior distributions, convergence diagnostics, and model assumptions for each model. Look for models that provide better parameter estimates, narrower posterior distributions, and better convergence properties. Visual inspection can help gain insights into the models' behavior and identify potential issues or improvements.

Of course, optimal selection depends on the goal of the analysis. Whilst causal inference might be the primary objective in one, model fit and prediction could be more relevant in another setting. Depending of the goal, different aspects are more important.

**Causal Inference:**

- Causal assumptions: Evaluate whether the models are designed to address the causal question of interest and adhere to appropriate causal assumptions. Consider factors such as confounding, selection bias, and causal identification strategies.

- Validity of estimates: Assess the validity of causal estimates from each model by examining the plausibility of assumptions, study design, and potential sources of bias.

- Interpretability: Consider the interpretability of the estimated causal effects in each model. Are the estimates meaningful and align with the causal question being addressed?


**Model Fit and Prediction:**

- Goodness of fit: Evaluate the models' goodness of fit to the observed data. Assess metrics such as the R-squared value, deviance, or other appropriate measures of model fit.

- Prediction accuracy: Consider the models' predictive performance on both training and test data. Compare metrics such as mean squared error, root mean squared error, or other relevant prediction measures.

- Model complexity: Consider the complexity of each model. Avoid overfitting by balancing the number of predictors, degrees of freedom, and model complexity with the available data.

It's essential to strike a balance between the causal inferential aspect and the model's fit and predictive capabilities. A model that fits the data well and provides accurate predictions might not necessarily capture the underlying causal relationships accurately. Conversely, a model that addresses causal inference comprehensively might not have the best fit or predictive performance.

Ultimately, the selection of the best model should be based on a careful consideration of both causal inference and model fit/prediction, as well as the specific goals and requirements of your analysis. It's often helpful to assess models from multiple perspectives and use multiple criteria to make a well-informed decision.


In our example, we use Bayesian model comparison to determine the best fitting model. 

**Bayesian model comparison**

```{r,warning=FALSE}
model_names <- c("Model 1","Model 2", "Model 3", "Model 4")

waic_blm1 <- waic(blm1)
waic_blm2 <- waic(blm2)
waic_blm3 <- waic(blm3)
waic_blm4 <- waic(blm4)
waic_values <- c(waic_blm1$estimates[3,1],waic_blm2$estimates[3,1],waic_blm3$estimates[3,1],waic_blm4$estimates[3,1])
names(waic_values) <- model_names

loo_blm1 <- loo(blm1)
loo_blm2 <- loo(blm2)
loo_blm3 <- loo(blm3)
loo_blm4 <- loo(blm4)
loo_values <- c(loo_blm1$estimates[3,1],loo_blm2$estimates[3,1],loo_blm3$estimates[3,1],loo_blm4$estimates[3,1])
names(loo_values) <- model_names

waic_values
loo_values

model_names[which.min(waic_values)]
model_names[which.min(loo_values)]
```

According to both the WAIC and the LOO, model 4 is the most appropriate. It is relatively complex so even though it might be good for forecasting, a simpler model might be better for causal inference. 

